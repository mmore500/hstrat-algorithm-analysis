Absent indefinite storage capacity, any piece of incoming streaming data must eventually be either evicted or digested if space is to be made available for new input \citep{gaber2005mining}.
This constraint is a crucial consideration in algorithm design for data streams, scenarios involving read-once inputs available only in a strictly ordered sequence.
Such streams' ordering may be dictated by inherently real-time processes (e.g., sensor readings) or retrieval limitations of storage media (e.g., a tape archive) \citep{henzinger1998computing}.
The data streaming model assumes input greatly exceeds memory capacity, with many analyses simply treating streams as unbounded \citep{jiang2006research}.

Data streaming scenarios pervade domains across science and industry \citep{aggarwal2009data,akidau2015dataflow}.
Commercial application areas include sensor networks \citep{elnahrawy2003research}, big-data analytics \citep{he2010comet}, real-time network traffic analysis \citep{johnson2005streams,muthukrishnan2005data}, systems administration \citep{fischer2012real}, and financial analytics for fraud prevention and algorithmic trading \citep{rajeshwari2016real,agarwal2009faster}.
Notable scientific applications arise in environmental/climate monitoring \citep{hill2009real} and astronomy \citep{graham2012data}.
Purely-programmatic computation can also behave as a data stream --- iterative simulation processes traverse vast expanses of ephemeral intermediate state that must be traced to verify simulation dynamics and assess simulation outcomes \citep{abdulla2004simulation,schutzel2014stream}.

Indeed, this broad utility has begat an extensive corpus of data stream algorithms.
Common objectives include rolling summary statistic calculations \citep{lin2004continuously}, on-the-fly data clustering \citep{silva2013data}, live anomaly detection \citep{cai2004maids}, and rolling event frequency estimation \citep{manku2002approximate}.
Data stream algorithms typically draw on one or more of three key stratagems: (1) rolling mechanisms, which restrict consideration to a FIFO tranche of recent data, (2) accumulation, which successively folds data into a summary statistic (e.g., sum, count, etc.) where data is repeatedly applied to a fixed amount of memory or resources, and (3) binning, which consolidates data within time interval bins to create a coarsened record.

Here, we focus on the third stratagem, binning.
Specifically, we develop efficient procedures to maintain temporally-representative subsamples of a data stream on a rolling basis.
That is, to read sequential observations from a data stream on an ongoing basis and sequence their disposal to maintain a record of data stream observations.

We term the rolling management of samples subsetted from a data stream as ``stream curation.''
Proposed algorithms span several possible requirements for two curatorial properties: (1) ``order of growth'' --- how curated collection size should grow in proportion to stream depth and (2) ``gap size bounds'' --- how retained samples should be spaced across stream history.
These considerations arise in various combinations across existing work \citep{aggarwal2003framework,han2005stream}, reviewed in detail later on;
here, we systematize these curational properties and contribute novel curatorial policy implementations distinguished by efficiency.
Each contributed policy includes indexing schemes that simultaneously support both efficient update operations and efficient storage of retained stream values in a flat array, requiring only $O(1)$ storage overhead --- a single counter value.

% @ELD TODO also cite JOSS paper
Although we do not treat it directly here, the original motivating application for contributed stream curation algorithms is ``hereditary stratigraphy,'' a recently-developed technique for distributed tracking of copy trees among replicating digital artifacts \citep{moreno2022hereditary}.
Applications of such tracking include phylogenetic analysis of highly-distributed genetic algorithms and evolutionary simulations, and provenance analysis of decentralized social network content, peer-to-peer file sharing, and computer viruses.
A brief description of hereditary stratigraphy is instructive to the timbre of algorithms contributed here.

\subsection{Hereditary Stratigraphy}

In order to reconstruct histories of relatedness, hereditary stratigraphy annotates replicating artifacts with a record of checkpoint fingerprints that grows by accretion with each replication event.
Comparing two artifacts' checkpoint records tells the extent of their common ancestry, as annotations will share common fingerprints up through the time of their last common ancestor and then differ.

Considering generational fingerprint records as a data stream, hereditary stratigraphy applies binning techniques to manage fingerprint accretion --- paring down retained fingerprints while maintaining checkpoints spaced across generations back to the progenitor artifact.
In the context of hereditary stratigraphy, stream curation decides how annotation size scales with generations elapsed by controlling how many retained strata accumulate.
Stream curation decisions directly influence capability for ancestry inference, because the onset of lineage divergence can only be discerned where fingerprints are retained.
Requirements on space usage and inferential power differ substantially between use cases of hereditary stratigraphy, so flexible support for a variety of record size/inferential power trade-offs is crucial.

Of particular note, however, is hereditary stratigraphy's necessity for compact representation of fingerprint records.
Because reduced fingerprint size allows more fingerprints to be retained, typical use will take fingerprints as individual bits, or possibly bytes (to avoid addressability complications).
In this context, representational overhead incurred, e.g., by explicitly storing fingerprints' individual stream sequence indices, can easily bloat annotations' footprint severalfold.
For some use cases, annotated artifacts will number millions or higher, so annotation inefficiency may substantially burden memory, storage, and network bandwidth (i.e., serialized artifact-annotation exchange).

Here, however, we present these algorithmic foundations developed for hereditary stratigraphy in the more generalized frame of data stream processing.
We describe a suite of indexing schemes for stream curation that support (1) linear, logarithmic, and constant scaling relationships between record size and generations elapsed and (2) both even-time and recency-biased distributions of retained stream items.
Implementations provided for each abate representational overhead for curated stream data to a single counter value.
Presented algorithms are published through the \texttt{hstrat} Python package for hereditary stratigraphy \citep{moreno2022hstrat}, but can be directly accessed through public APIs fully independent of other aspects of hereditary stratigraphy methodology.

To provide further introduction to key concepts behind stream curation, the next sections situate our proposed stream curation procedures within existing data stream literature and consider applications of stream curation data loggers and sensor networks.
