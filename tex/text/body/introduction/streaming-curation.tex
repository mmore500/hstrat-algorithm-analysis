\subsection{Stream Curation} \label{sec:streaming-curation}

Under an iterative model, the passing of time operates something like a ``first-in, nothing-out'' queue --- successive time steps simply pile on ad infinitum.
As time accumulates, each elapsed time step recedes ever deeper.
A discrete event's absolute time point does not change, but its relation to the present does.  % cite xckd 1477?
This inevitability is crux to the ``stream curation'' problem, which we establish to describe rolling maintenance of a temporally representative cross-section of data stream observations.
% For the sake of generality, we frame stratum retention strategies for hereditary stratigraphy in terms of stream curation in this paper.

Stream curation algorithms must answer how many observations should be kept at any point in time, but also how observations that are retained should be spaced out over past time.
Appropriate choices vary by use case, and no stream curation policy can meet all possible demands.
For this reason, we consider a spectrum across two factors: size limitation, i.e., how many observations may be retained, and resolution guarantees, i.e., maximum gap sizes.
For each space-vs-resolution trade-off explored, we provide an implementation algorithm meeting criteria of computational reducibility and self-consistency, defined below.
We first discuss stratum curation policy trade-off stipulation criteria and policy implementation for stream curation, then close with connections of stream curation to existing work and potential applications.

\subsubsection{Stream Curation Policy Stipulation}

Two primary dimensions of stream curation policy matter for practical purposes: 1) retained collection size and 2) time gap sizes between retained observations.

% To specify requirements on retained collection size,
% @ELD: is it okay to drop mention of observation count for readability or will this cause problems?
We bound retained collection size to a fixed value or a function of time elapsed.
Asymptotic bounds on the scaling relationship between collection size and time are the ``size order of growth.''
In some cases, we also define hard bounds, referred to as a ``size cap,'' in light of practical considerations; for example, a user may have a fixed size memory allocation in which to store a curated collection.

% Specification of gap size requirements must consider a second factor in addition to time elapsed.
Bounds on spacing between retained observations, a ``resolution guarantee,'' should depend on both the elapsed time and the temporal depth of a particular observation.
Taking into account historical depth allows skew in retained observation density.
For instance, observations may be retained at evenly-spaced time points or, alternately, thinned proportionately to historical depth.
The latter approach biases observational detail to recent time, which may be important in some use cases.

% @ELD could cut this paragraph
For example, in the context of hereditary stratigraphy, recency-proportional resolution is typically preferable.
Coalescent theory predicts a tendency for evolution-like processes to produce phylogenies with many recent branches and progressively fewer ancient branches \citep{nordborgCoalescentTheory2019, berestyckiRecentProgressCoalescent2009}.
Thus, fine inferential detail over recent time points usually proves more informative to phylogenetic reconstruction than detail over more ancient time points.
Indeed, trials reconstructing known lineages have found that recency-skewing retention provides better quality reconstructions \citep{moreno2022hereditary}.

Note that size bounds and resolution guarantees must hold across all time points for use cases where observation collections will see sustained use over time or the endpoint for an observation collection is indeterminate (e.g., computations with a real-time termination condition).
This factor obliges policy design nuance: if resolution guarantees shift as generations elapse and observations become more ancient, cohorts of retained strata must, in dwindling, morph through a constrained series of retention patterns.

\subsubsection{Stream Curation Policy Algorithms}

A stream curation policy algorithm produces a sequence of retained observation sets, one for each time point when the underlying data stream is sampled.
Policy algorithms must meet several requirements.

First and foremost, each of an algorithm's retention sets should satisfy all stipulated requirements on collection size and gap size.

Additionally, to be viable, each retention set must be a subset of all preceding retention sets.
Otherwise, a previously discarded observation would be selected for inclusion, which is impossible (once data is discarded it cannot be retrieved).
We call this property self-consistency.

For the sake of efficient operation, we impose a final nuts-and-bolts requirement on algorithm implementation: computational reducibility, meaning that observation times retained at any point must be directly enumerable.
This capability enables observations' time points to be deduced positionally from a buffer index, so observation times may be omitted.
In the context of hereditary stratigraphy --- where e.g., observations are single bits or bytes --- several-fold space savings may result.

Memory savings from computational reducibility can matter greatly.
Since the austere early days of computing, typical hardware has trended away from resource scarcity \citep{kushida2015cloud}, yet memory efficiency remains crucial in certain contexts where hardware trends have stagnated or even regressed memory capacity.

% @ELD could cut next couple paragraphs
Aspects of high-performance computing (HPC) expect to continue scaling out with lean processing cores \citep{sutter2005free,morgenstern2021unparalleled}.
The Cerebras Wafer-Scale Engine (WSE) epitomizes this trend, packaging an astounding 850,000 computing elements onto a single die.
Individual WSE cores, however, have just 48kb of memory and can only communicate within a local mesh \citep{cerebras2021wafer,lauterbach2021path}.

Component economization and minaturization has also influenced the Internet of Things (IoT) revolution \citep{rfc7228,ojo2018review}, an ongoing march of ubiquitization potentially culminating in a ``smart dust'' of downscale, low-end hardware \citep{warneke2001smart}.
The Michigan Micro Mote platform for instance, provisions a mere 3kb of retentive memory within its cubic millimeter form factor \citep{lee2012modular}.
More recent work has explored devices tucked within dandelion-like parachutes \citep{iyer2022wind}.
That chipset is yet more austere, provisioning 2 kilobytes of volatile flash memory --- and a mere 128 bytes of retentive memory \citep{microchip2014atiny20}.
As engineers continue to plumb the extremities of technical feasibility, bare-bones computing modalities will persist, and necessitate lightweight data stream algorithms such as ours.

\subsubsection{Existing Work Related to Stream Curation}

Stream curation closely relates to existing binning procedures that group together and consolidate contiguous subsections of a data stream.

The fixed-resolution policy algorithm presented in Section \ref{sec:fixed-resolution-algo} is simple down sampling via decimation \citep[p. 31]{crochiere1983multirate}.
Our depth-proportional resolution (Section \ref{sec:depth-proportional-resolution-algo}) and recency-proportional resolution (Section \ref{sec:recency-proportional-resolution-algo}) algorithms share close structural similarity with the online equi- and vari-segmented schemes proposed in
\citep{zhao2005generalized}.
The depth-proportional resolution structure has appeared additionally in ``pyramidal'' and ``tilted'' time window schemes \citep{aggarwal2003framework,han2005stream}.

To our knowledge, these previous implementations all unfold through stateful iteration, with representational overhead for each stored value (viz. segment length values); stateless enumerations of retained set composition are original to our work in this paper.
We are also not aware of existing equivalents or near-equivalents of the presented geometric sequence $n$th root and curbed recency-proportional resolution policy algorithms (Sections \ref{sec:geom-seq-nth-root-algo} and \ref{sec:curbed-recency-proportional-resolution-algo}).

Work on ``amnesic approximation'' tackles a similar end goal, but has only loose technical overlap.
\cite{palpanas2004online} provides a generalized scheme to incrementally down-sample a data stream pursuant to a user-defined time-back-to-value function by means of a stateful iterative process.

\subsubsection{Applications of Stream Curation}

Correspondences between stream curation and more general binning on data streams suggest avenues for application of stream curation policy algorithms across data stream scenarios.

Perhaps most plainly, the stream curation down-sampling problem parallels those faced by unattended data logger devices that manage incoming observation streams, often on an indefinite or indeterminate basis.
Devices incorporated into wireless sensor networks may also experience irregular device uplink schedules.
The ``mobile sink'' paradigm \citep{jain2022survey}, for example, relies on network base station(s) that physically traverse the coverage area and uplink sensor nodes on potentially sporadic patrol schedules.

Existing work has largely applied rolling full retention of most recent data within available buffer space \citep{fincham1995use} or dismissal of incoming data after storage reaches capacity \citep{saunders1989portable,mahzan2017design}.
Strategies to maintain a cross-sectional time sample appear scant, although there has been some work to extend the record capacity of data loggers through application-specific online compression algorithms \citep{hadiatna2016design}.
