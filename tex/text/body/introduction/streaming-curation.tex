\subsection{Stream Curation} \label{sec:streaming-curation}

Under an iterative model, time operates something like a first-in, nothing-out queue --- successive time steps simply pile on ad infinitum.
As time accumulates, an elapsed time step recedes ever deeper.
A discrete event's time point does not change, but its relation to the present does.  % cite xckd 1477?
This inevitability is crux to the stream curation problem, which we establish to describe rolling maintenance of a temporally representative cross-section of sequenced observations.
For the sake of generality, we employ this framing to discuss stratum retention strategies for hereditary stratigraphy.
stream curation relates directly to the concept of binning in data streams, which we discuss further below.

Stream curation algorithms must answer how many observations should be kept at any point in time, but also how retained observations should be spaced out over past time.
Appropriate choices vary by use case, and no stream curation policy can meet requirements of all use cases.
For this reason, we consider a spectrum of balances between two factors: size limitation, i.e., how many observations may be retained, and resolution guarantees, i.e., maximum gap sizes.
For each space-resolution stipulation profile, we provide an implementation algorithm meeting criteria of computational reducibility and self-consistency, defined below.
After discussing policy stipulation criteria and policy implementation for stream curation, we close with connections to existing work and potential applications.

\subsubsection{Stream Curation Policy Stipulation}

Functional requirements for stream curation policy delineate collection size, i.e., total observations retained, and gap size, i.e., length of time windows between retained observations.

For the former, we specify bounds on retained observation count as fixed or a function of time elapsed.
We refer asymptotic bounds on the scaling relationship between size and time as  the ``size order of growth.''
We also define hard bounds, referred to as ``size cap,'' vis-a-vis practical considerations, e.g., fixed size memory allocation for curated collection storage.

Bounds on spacing between retained observations, ``resolution guarantee,'' may depend on both the total number of generations elapsed but the historical depth of a particular time point in the stratigraphic record.
Again, we treat both asymptotic and hard bounds.
Considering historical depth provides for configurability of skew in observation density.
Strata from evenly-spaced time points may retain uniform detail over the entire range of elapsed time points.
Alternately, retention allocations may space successive retained observations proportionately to historical depth.
Such an approach biases observational detail to recent time.
Our surveyed policy stipulations include both even and recency-proportional resolutions.

In the context of hereditary stratigraphy, recency-proportional resolution is typically preferable.
Coalescent theory predicts a tendency for evolution-like processes to produce phylogenies with many recent branches and progressively fewer ancient branches \citep{nordborgCoalescentTheory2019, berestyckiRecentProgressCoalescent2009}.
Thus, fine inferential detail over recent time points is usually more informative to phylogenetic reconstruction than detail over more ancient time points.
Indeed, experiments reconstructing known lineages have shown that recency-skewing retention provides better quality reconstructions \citep{moreno2022hereditary}.

Note that size bounds and resolution guarantees must hold across all time points, necessitated by use cases where observation collections will see sustained use over time or the endpoint for an observation collection is indeterminate (e.g., computations with a real-time termination condition).
This factor introduces design subtlety: as generations elapse and observations become more ancient, resolution guarantees may shift.
In those cases, cohorts of retained strata must, in dwindling, gracefully morph through a constrained series of retention patterns.

\subsubsection{Stream Curation Policy Algorithms}

A stream curation policy algorithm produces a sequence of observation retention sets, one for each time point.
First and foremost, each of an algorithm's retention sets should satisfy stipulated requirements on collection size and gap size.

To be viable, each retention set's complement (i.e., discarded observations) must superset the complement of all preceding retention sets.
Otherwise, a discarded observation would be called for inclusion.
Put another way, policy algorithms must ensure no retained observations have been discarded at a prior time point.
We call this property self-consistency.

We impose a further nuts-and-bolts requirement on algorithm implementation: computational reducibility, meaning that time points retained at every position in the observation sequence must be directly enumerable.
This capability enables observations' time point to be deduced positionally, so observation time points may be omitted from an observation set's representation.
In the context of hereditary stratigraphy, several-fold space savings may result (depending on differentia bit width).

Since the austere early days of computing, typical hardware has trended away from resource scarcity, making lean memory use less salient \citep{kushida2015cloud}.
Nevertheless, memory efficiency remains crucial in certain contexts where hardware trends have stagnated or even regressed memory capacity,

High-performance computing (HPC) expects to continue emphasis of scale-out with lean processing cores rather than boosting capacities of individual processing components \citep{sutter2005free,morgenstern2021unparalleled}.
The Cerebras Wafer-Scale Engine epitomizes this trend, packaging an astounding 850,000 computing elements onto a single die.
Individual core, however, host just 48kb of memory and operate through a exclusively-local mesh communication model \citep{cerebras2021wafer,lauterbach2021path}.

As with HPC, component economization and minaturization influenced the Internet of Things (IoT) revolution \citep{rfc7228,ojo2018review}, a march potentially culminating in ``smart dust'' \citep{warneke2001smart} of downscale, low-end hardware.
The Michigan Micro Mote platform for instance, provisions a mere 3kb of retentive memory within its cubic millimeter form factor \citep{lee2012modular}.
More recent work has explored devices tucked within the form factor of dandelion parachutes \citep{iyer2022wind}.
The chipset is yet more austere, provisioning 2 kilobytes of volatile flash memory --- and a mere 128 bytes of retentive memory \citep{microchip2014atiny20}.
As engineers continue to plumb the extremities of technical feasibility, bare-bones computing modalities will persist, and applications in wireless sensor networks will necessitate lightweight data stream algorithms.

\subsubsection{Existing Work Related to Stream Curation}

Stream curation relates to existing binning procedures that group together and consolidate contiguous subsections of a data stream.

The fixed-resolution policy algorithm presented here is simple downsampling via decimation \citep[p. 31]{crochiere1983multirate}
Our depth-proportional resolution (Section \ref{sec:depth-proportional-resolution-algo}) and recency-proportional resolution (Section \ref{sec:recency-proportional-resolution-algo}) algorithms share close structural similarity with the online equi-segmented and vari-segmented schemes proposed in
\citep{zhao2005generalized}.
The depth-proportional resolution structure additionally appears as ``pyramidal'' and ``tilted'' time window schemes \citep{aggarwal2003framework,han2005stream}.
However, provided implementations all unfold through stateful iteration, with representational overhead for each stored value (viz. segment length values).
To our knowledge, stateless enumerations of retained set composition are original to this work.
We are also not aware of existing equivalents or near-equivalents of presented geometric sequence $n$th root or curbed recency-proportional resolution policy algorithms (Sections \ref{sec:geom-seq-nth-root-algo} and \ref{sec:curbed-recency-proportional-resolution-algo}).

Looser relations include work on ``amnesic approximation'' a generalized scheme to incrementally down sample a data stream pursuant to a user-defined time-back-to-value function.

\subsubsection{Applications of Stream Curation}

The correspondences between stream curation and more general binning on data streams open avenues apply stratum curation policy algorithms in various data stream scenarios.

Perhaps most plainly, the down sampling task faced by hereditary stratigraph annotations parallel those faced by data logger devices.
Unattended loggers manage incoming observation streams on a potentially indefinite or indeterminate basis.
Devices incorporated into wireless sensor networks also experience irregular device uplink schedules.
The ``mobile sink'' paradigm \citep{jain2022survey}, for example relies on network base station(s) physically traverse the coverage area and transact with nearby sensor nodes.
Sporadic patrol schedules can thus introduce uncertainty in data transfer schedules.

Reported strategies to handle stream data in excess of storage capacity on loggers include rolling full retention of most recent data within available buffer space \citep{fincham1995use} and dismissal of incoming data after storage reaches capacity \citep{saunders1989portable,mahzan2017design},
Strategies to maintain a cross-sectional time sample do not appear to be widely exploited, although there has been some work to extend the record capacity of data loggers through application-specific online compression algorithms \citep{hadiatna2016design}.
Storage-limited platforms might suit proposed memory-efficient stream curation procedures.
