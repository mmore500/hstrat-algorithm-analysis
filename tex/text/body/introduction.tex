\section{Introduction} \label{sec:introduction}

Absent indefinitely provisioned storage capacity, at some point data eviction or digestion becomes necessary to continue processing inputs \citep{gaber2005mining}.
This constraint is core to algorithm design for data streams, scenarios involving read-once inputs available in a strictly determinate sequence.
In data streams, ordering may be dictated by inherently real-time processes (e.g., sensor readings) or retrieval limitations of storage media (e.g., a tape archive) \citep{henzinger1998computing}.
The data streaming model assumes input greatly exceeds memory capacity, with many analyses simply treating streams as unbounded \citep{jiang2006research}.

Data streaming scenarios pervade domain contexts across science and industry \citep{aggarwal2009data,akidau2015dataflow}.
Commercial application areas include sensor networks \citep{elnahrawy2003research}, big-data analytics \citep{he2010comet}, real-time network traffic analysis \citep{johnson2005streams,muthukrishnan2005data}, systems administration \citep{fischer2012real}, and financial analytics for fraud prevention and algorithmic trading \citep{rajeshwari2016real,agarwal2009faster},
Notable scientific applications arise in environmental/climate monitoring \citep{hill2009real} and astronomy \citep{graham2012data}.
Purely-programmatic computation can also behave as a data stream --- iterative simulation processes traverse vast expanses of ephemeral intermediate state that must be traced to verify simulation dynamics and assess simulation outcomes \citep{abdulla2004simulation,schutzel2014stream}.

Indeed, this broad utility has mustered an extensive corpus of algorithms that sip from data streams.
Typical objectives include running summary statistics calculations \citep{lin2004continuously}, on-the-fly data clustering \citep{silva2013data}, live anomaly detection \citep{cai2004maids}, and rolling event frequency estimation \citep{manku2002approximate}.
Across data stream algorithms, approaches typically draw on three key stratagems to solve the mismatch between data volume and working storage capacity: (1) rolling mechanisms, which restrict consideration to a FIFO tranche of recent data, (2) accumulation, which successively folds data into a summary statistic (e.g., sum, count, etc.) where data is repeatedly applied to a fixed amount of memory or resources, and (3) binning, which consolidates data within time intervals spaced across a stream's history to create a coarsened record.

Here, we focus on the third stratagem, binning, and apply it to distributed provenance tracking for replicating digital artifacts using a technique we call ``hereditary stratigraphy.''
Our motivating application for tracking copy trees is phylogenetic analysis of genetic algorithms and evolutionary simulations, but other areas of interest include decentralized social network content, distributed systems messaging, peer-to-peer file sharing, and computer viruses.
In order to reconstruct histories of relatedness, we annotate replicating artifacts with a record of checkpoint fingerprints that grows by accretion with each replication event.
Comparing two checkpoint records tells the extent of common ancestry between two artifacts, which will share common fingerprints up through their last common ancestor and then differ.
We consider the fingerprint record as a data stream, and apply binning techniques to manage fingerprint accretion, paring down retained fingerprints while maintaining checkpoints spaced across time back to the progenitor artifact.
In accordance with the technique's geological metaphor, individual fingerprints are referred to as ``strata.''

We describe such rolling subset management as a ``streaming curation'' process on a data stream.
In the context of hereditary stratigraphy, streaming curation decides how annotation size scales with generations elapsed by dictating how many retained strata accumulate.
Streaming curation also governs inference of artifact ancestry by deciding the subset of historical time points where lineage divergence can be discerned.
Requirements on space usage and inferential power differ substantially between use cases, making flexible support for a variety of size/power trade-offs crucial.

Hereditary stratigraphy's tractability hinges on compact representation of binned stream records.
For some use cases, annotated artifacts will number millions or higher.
Thus, annotation inefficiency may substantially burden memory, storage, and network bandwidth (i.e., serialized artifact-annotation exchange).
Because reduced fingerprint size boosts capacity for temporal density of retained fingerprints, typical use will take fingerprints as individual bits, or possibly bytes to avoid addressability complications.
In this context, representational overhead incurred, e.g., by explicitly storing fingerprints' individual stream sequence indices, can easily bloat annotations' footprint severalfold.

Here, we present a suite of indexing schemes that reduce representational overhead for fingerprint records to a single generation-count value.
These schemes support linear, logarithmic, and constant scaling relationships between record size and generations elapsed and both even-time and recency-biased distributions of retained fingerprints.
Finally, we explain an efficient procedure to integrate an ancestry tree (i.e., a phylogeny) for large collections of annotated artifacts.
These contributions lay algorithmic foundations for decentralized provenance tracking, but also pertain more broadly to data stream processing.

The next sections recap the principles and applications of hereditary stratigraphy, considerations in applying streaming curation to stratum retention in hereditary stratigraphy, and then situate streaming curation procedures within existing data stream literature and consider applications in data loggers and sensor networks.

\subsection{Hereditary Stratigraphy}

\subsection{Stratum Retention}

\subsection{Streaming Curation}

Memory-efficient representation developed for hereditary stratigraphy can benefit other applications of stream curation and more general binning on data streams.
Indeed, hardware trends promise to expand operational scenarios requiring memory-critical data stream processing.
High-performance computing (HPC) expects to continue emphasis of scale-out with lean processing cores rather than boosting capacities of individual processing components \citep{sutter2005free,morgenstern2021unparalleled}.
The Cerebras Wafer-Scale Engine epitomizes this trend, packaging an astounding 850,000 computing elements onto a single die.
Individual core, however, host just 48kb of memory and operate through a exclusively-local mesh communication model \citep{cerebras2021wafer,lauterbach2021path}.

As with HPC, component economization and minaturization influenced the Internet of Things (IoT) revolution \citep{rfc7228,ojo2018review}, a march potentially culminating in ``smart dust'' \citep{warneke2001smart} of downscale, low-end hardware.
The Michigan Micro Mote platform for instance, provisions a mere 3kb of retentive memory within its cubic millimeter form factor \citep{lee2012modular}.
More recent work has explored devices tucked within the form factor of dandelion parachutes \citep{iyer2022wind}.
The chipset is yet more austere, provisioning 2 kilobytes of volatile flash memory --- and a mere 128 bytes of retentive memory \citep{microchip2014atiny20}.
As engineers continue to plumb the extremities of technical feasibility, bare-bones computing modalities will persist, and applications in wireless sensor networks will necessitate lightweight data stream algorithms.

%% TODO continue revisions

Lightweight

There has been some work to extend the record capacity of data loggers through application-specific online compression algorithms \citep{hadiatna2016design}, but to our knowledge the streaming curation problem has not been treated directly from either a theoretical or empirical vantage.




A common question being whether to distribute bin density evenly over history or to bias information to recent events.

running or rolling, where you just consider the latest data, and binning, where you group and summarization
Among data streOne common device.
Across data stream algorithms, common operations include A, B, and C


windowing (tilted-time) natural time \citep{giannella2003mining}

dimension-reduction \citep{zhao2005generalized}
amnesic functions \citep{palpanas2008streaming}
We refer to it as the stream curation problem,


@article{zhao2005generalized,
  title     = {Generalized dimension-reduction framework for recent-biased time series analysis},
  author    = {Zhao, Yanchang and Zhang, Shichao},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  volume    = {18},
  number    = {2},
  pages     = {231--244},
  year      = {2005},
  publisher = {IEEE}
}
@inproceedings{palpanas2004online,
  title        = {Online amnesic approximation of streaming time series},
  author       = {Palpanas, Themistoklis and Vlachos, Michail and Keogh, Eamonn and Gunopulos, Dimitrios and Truppel, Wagner},
  booktitle    = {Proceedings. 20th International Conference on Data Engineering},
  pages        = {339--349},
  year         = {2004},
  organization = {IEEE}
}
@article{palpanas2008streaming,
  title     = {Streaming time series summarization using user-defined amnesic functions},
  author    = {Palpanas, Themis and Vlachos, Michail and Keogh, Eamonn and Gunopulos, Dimitrios},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  volume    = {20},
  number    = {7},
  pages     = {992--1006},
  year      = {2008},
  publisher = {IEEE}
}
@inproceedings{aggarwal2003framework,
  title        = {A framework for clustering evolving data streams},
  author       = {Aggarwal, Charu C and Philip, S Yu and Han, Jiawei and Wang, Jianyong},
  booktitle    = {Proceedings 2003 VLDB conference},
  pages        = {81--92},
  year         = {2003},
  organization = {Elsevier}
}

]
Indeed, many algorithms to sip from data streams have been developed.


We leverage these algorithms and extend them to create hyper-compact representations amenable to produce a technique for distributed streaming data-lineage tracking.
Hyper-compact
- zero overhead (no stored timestamps or data pointer/dividers)
- efficient update rules
- O(1) storage options
We provide C++ and Python implementations in the accompanying \texttt{hstrat} library \citep{moreno2022hstrat}.


Why hyper-compact (zero-overhead?) Annotations attached to individual data elements with data items on the order of individual bytes or bits.


We term this ``Data Stream Curation''

We encountered this in tracking phylogenies of replicating digital artifacts.
In this paper, we start out by treating streaming curaiton.

Hyper-compact representations are important in other in other scenarios: IoT and accelerator-drive HPC.

Early days of computing were marked by resource scarcity.
As desktop computing has advanced, this becomes less salient.

For the sake of generalizability, Section \ref{sec:annotation-algorithms} organizes presentation of stratum retention algorithms through the lens of the streaming curation problem.

% autocannabilism https://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.8
% visual sedimentation https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6634152

Closely related to tilted-time sampling \citep{giannella2003mining,han2005stream}

\citep{zhao2005generalized} <-- merging windows

\citep{palpanas2004online} <-- custom amnesiac function



Digital evolution is a field of research that sits at the intersection of evolutionary biology and machine learning.
By instantiating evolution in computers, researchers are able to advance our knowledge of both subjects.
Increasing computational power is essential to many advances in digital evolution and, more broadly, artificial life \citep{ackley2014indefinitely}.
In fact, a central motif in the field --- the problem of open-ended evolution, which asks how and if closed systems can indefinitely yield new complexity, novelty, and adaptation --- is inexorably intertwined with computational scale, and has been implicated as meaningfully compute-constrained in at least some systems \citep{taylor2016open,channon2019maximum}.
It is not an unreasonable possibility that orders-of-magnitude changes in computational power could induce qualitative changes in digital evolution and artificial life systems \citep{moreno2022engineering}, analogously to the renaissance of deep learning with the advent of GPGPU computing \citep{krizhevsky2012imagenet}.

% ELD: TODO: consider cutting this paragraph
Parallel and distributed computing is, indeed, widely leveraged in digital evolution.
Applications range from processing entirely independent evolutionary replicates across compute jobs \citep{dolson2017spatial, hornby2006automated}, to data-parallel fitness evaluation of single individuals over independent test cases using hardware accelerators \citep{harding2007fast_springer, langdon2019continuous}, to application of primary-subordinate/controller-responder parallelism to delegate costly fitness evaluations of single individuals \citep{cantu2001master,miikkulainen2019evolving}.
In recent years, Sentient Technologies spearheaded evolutionary computation projects on an unprecedented computational scale, comprising over a million heterogeneous CPUs from time-available providers capable of a peak performance of 9 petaflops \citep{miikkulainen2019evolving,gilbert2015artificial,blondeau2009distributed}.

Although highlighted as key to the future of the field, fully decentralized, highly-distributed approaches such as island models \citep{bennett1999building,schulte2010genetic} and mesh-computing approaches prioritizing dynamic interactions \citep{ray1995proposal,ackley2018digital,moreno2021conduit} have been less thoroughly developed.
A major barrier to effective applications of the fully-distributed paradigm is the difficulty of observing the state of the system.
In the absence of a global population-level view, the course of evolution becomes much more challenging to study --- undercutting the objective of these systems as an experimental tool.
A prime example of this problem is the difficulty of recording phylogenies (i.e. ancestry trees) in distributed systems.

\subsection{Phylogeny recording}

Although phylogenetic analysis is a powerful tool for understanding digital evolution \citep{dolson2020interpreting}, typical approaches are particularly difficult to scale. %poses a particularly problematic stumbling block.
Most existing analyses use direct lineage tracking, where a centralized data structure collates individual reproduction events to produce a complete, perfect phylogenetic record \citep{dolson2023phylotrackpy}.
However, this approach requires centralized data storage which performs poorly in distributed computing environments (see Sections \ref{sec:perfect-tracking-distrbuted} and \ref{sec:perfect-tracking-pruning-distrbuted} for more information).

The problem of recording phylogenies in a distributed context generalizes beyond digital evolution.
Phylogenies can be used to understand any process in which a population of objects exist and those objects are sometimes replicated (with or without modification).
Indeed, phylogenetic techniques have been used to study the routes through which digital image misinformation and computer viruses spread \citep{friggeri2014rumor,cohen1987computer}.
As in digital evolution, such studies generally rely on centralized tracking and are thus inhibited when it is not possible.
As a notable exception, Libennowell et al. used decentralized methods to trace global dissemination of chain emails.
Intriguingly, discussed further below, this work exploit sa serendipitous instance of the mechanistic principle our proposed algorithms exploit \cite{libennowell2008tracing}.

As such, hereditary stratigraphic techniques certainly possess some potential afford experimenters visibility into the otherwise cloistered processes through which such digital artifacts spread.
Notably, though, such experiments would predicate on some level of influence over the artifact copy process in to ensure dispatch of the hereditary stratigraphic update process and the absence of actors with motivation and capability to antagonistically deface annotation data.


% TODO also tie in the algorithmic analysis of phylogenies here
% ELD: TODO: See if any of the points below aren't made in the perfect tracking section, and if not move them
% However, long distance, potentially many-hop, transmission of reproduction event records to a centralized data store introduces potentially significant amounts of communication overhead and bottlenecking.
% The necessity to propagate extinction notifications at runtime so that records of lineages without extant descendants can be pruned away to prevent runaway memory use imposes additional communication overhead.
% Further, at large enough scales, node failures become inevitablies rather than inconveniences and, at even larger scales, the cost of attempting full recovery from node dropout to prevent data loss could become high enough to conceivably preclude forward simulation progress \citep{cappello2014toward}.
% Within the perfect-tracking paradigm, individual missing relationships could potentially entirely disjoin knowledge of how large portions of phylogenetic history relate.
% This sensitivity to data loss poses a second major challenge in scaling perfect tracking.

Looking to natural systems, it is clear that robust phylogenetic analyses can be conducted on data generated through fully-distributed processes.
Inspired by systematic biology, we can consider post-hoc inference from heritable data as an alternative tocentralized tracking.

Although such analyses can be accomplished solely from pre-existing, functional digital genome contents \citep{moreno2021case}, such algorithms do not generalize across diverse digital evolution systems and suffer from many of the complications that plague phylogenetic analyses of biological systems.
An intriguing proposition arises: how might a genetic representation be designed to maximize ease and efficacy of phylogenetic reconstruction?
If sufficiently compact, such a representation could be attached as a non-functional annotation to facilitate phylogenetic inference over genomes of interest.
This line of reasoning led to the recent development of ``hereditary stratigraphy'' methodology, which associates digital genomes with heritable annotations specially designed to provide universal system-invariant phylogenetic reconstruction with well-defined, tunable expectations for inference accuracy
\citep{moreno2022hereditary}.


\subsection{Hereditary stratigraphy}

The crux of hereditary stratigraphy is that each at generation, a new fixed-length packet of randomly-generated data is appended to inherited annotations.
These stochastic ``fingerprints,'' typically sized on the order of a few bits or bytes, serve as a sort of checkpoint for lineage identity at a particular generation.
At future time points, extant annotations will share identical fingerprints for the generations they experienced shared ancestry.

Interestingly, a similar technique was used by \cite{libennowell2008tracing} to trace global dissemination of chain emails.
These researchers applied \textit{post-hoc} methods to reconstruct estimated phylogenies of the propagation of two chain mail messages.
These phylogenies then served as a reference to tune agent-based models, ultimately yielding a remarkable elucidation of email user behavior and underlying social dynamics.
Interestingly, this study's reconstructions were solely enabled by a special peculiarity of the two sampled messages: they were email petitions.
Thus, users would append their name to the list before forwarding it on --- a mechanism strikingly similar to the broad strokes of hereditary stratigraphy.

Naive application of hereditary stratigraphy would produce indefinitely lengthening genome annotation size in direct proportion to generations elapsed.
To overcome this obstacle, hereditary stratigraphy prescribes a ``pruning'' process to discard strata on the fly as new strata are deposited.
Pruning reduces annotation size, but at the expense of introducing inferential uncertainty: the last generation of common ancestry between two lineages can be resolved no finer than the time points associated with retained strata.
Deciding which strata to discard is the main point of algorithmic interest associated with the technique.
In fact, this scenario turns out to be an instance of a more general class of online algorithmic problems we hereby term as ``streaming curation.''
In the context of hereditary stratigraphy, we refer to the decision-making rules that decide which retained strata to discard per generation as a ``stratum retention'' algorithm.

Putting ``hereditary stratigraphy'' techniques into practice requires a point of further consideration.
Naive application of the approach described above would entail indefinitely lengthening genome annotation size in direct proportion to generations elapsed due to the creation of new ``fingerprint'' data each generation.
To overcome this obstacle, hereditary stratigraphy prescribes a ``pruning'' process to discard strata on the fly as new strata are deposited.
Note that pruning reduces annotation size, but at the expense of introducing inferential uncertainty: the last generation of common ancestry between two lineages can be resolved no finer than the time points associated with retained strata.
Deciding which strata to discard when proves to be the main point of algorithmic interest associated with the technique.
In fact, as will be delved into a little further on, we find this scenario to be an instance of a more general class of online algorithmic problems we term as ``streaming curation.''
In the context of hereditary stratigraphy, we refer to the decision-making rules that decide which retained strata to discard per generation as a ``stratum retention'' algorithm.


\subsection{Stratum retention}

The obvious core issue of stratum retention is how many strata to discard.
In many applied cases, it is desirable to keep the count of retained strata at or below a size cap indefinitely as generations elapse.
However, in some cases discussed later on, it may be desirable to allow a logarithmic growth rate of annotation size to guarantee upper bounds on inferential uncertainty.

Determining stratum retention strategy also raises a more subtle second consideration: what skew, if any, to induce on the composition of retained strata.
Strata from evenly-spaced time points may be retained in order to provide uniform inferential detail over the entire range of elapsed time points.
However, coalescent theory predicts that evolution-like processes will tend to produce phylogenies with many recent branches and progressively fewer more ancient branches \citep{nordborgCoalescentTheory2019, berestyckiRecentProgressCoalescent2009}.
Thus, fine inferential detail over more recent time points is usually more informative to phylogenetic reconstruction than fine detail over more ancient time points.
Thus, among a fixed-size retained sampling of time points, skewing the composition of retained strata to over-represent more recent time points would likely provide better bang for the buck with respect to reconstructive power.
Indeed, experiments reconstructing known lineages have shown that recency-skewing retention provides better quality reconstructions \citep{moreno2022hereditary}.
So, in addition to evenly-spaced retention, we consider retention allocations that yield gap widths between successive retained strata (and corresponding estimation uncertainty) scaled proportionately to those strata's depth back in history.

Because no single retention policy can meet requirements of all use cases, we present in Section \ref{sec:annotation-algorithms} a suite of complementary stratum retention algorithms covering possible combinations of retained stratum count order of growth and chronological skew.
We describe retention algorithms in terms of upper bounds on memory usage and inference uncertainty.

With respect to memory usage, we refer to guaranteed upper bounds as ``size order of growth'' in the asymptotic case with respect to elapsed generations or ``size cap'' in the absolute case.
We refer to bounds on spacing between retained strata a ``resolution guarantee.''
Resolution guarantee specification incorporates both the total number of generations elapsed and the historical depth of a particular time point in the stratigraphic record --- so, bounding is tailored within these particular circumstances.

%ELD: TODO: This said "three nutes and bolts consdierations" but there were only two. I changed it to say two, but is there one missing?
In addition to their particular size bounds and resolution guarantees, we demonstrate all proposed stratum retention algorithms satisfy two nuts and bolts algorithmic considerations:
\begin{enumerate}
\item \textbf{Tractability of directly enumerating deposition time of retained strata at any arbitrary generation.} Efficient computation of the deposition times retained at each time point provides a tractable reverse mapping from column array index to deposition generation.
  Such a mapping enables deposition generation to be omitted from stored strata, potentially yielding several-fold space savings (depending on the differentia bit width used).

\item \textbf{Stratum discard sequencing for ``self-consistency.''} 
  When you discard a stratum now, it won't be available later.
  If you need a stratum at a particular time point, you must be able to guarantee it hasn't already been discarded at any prior time point.

\end{enumerate}

% As a final spelling out of a critical conceptual point in discussion of stratum retention policies, note that size bounds and resolution guarantees are enforced across across all generations.
% This means stratum retention policies must manage ``online'' column composition across rolling generations.
% Indeed, for many use cases, resolution and column size guarantees will need to hold at all generations because the number of generations elapsed at the end of an experiment is often not known \textit{a priori} and the option of continuing a fixed-length experiment with evolved genomes is desired.
% This factor introduces a design subtlety: as generations elapse, deposited strata recede to increasingly ancient historical depth with respect to the current generation.
% Resolution guarantees may change along the way back.
% In those cases, cohorts of retained strata must, in dwindling, gracefully morph through a constrained series of retention patterns.

Note that size bounds and resolution guarantees are enforced across across all generations.
Thus, stratum retention policies must manage ``online'' column composition across rolling generations.
Indeed, for many use cases, resolution and column size guarantees will need to hold at all generations because the number of generations elapsed at the end of an experiment is often not known \textit{a priori} and the option of continuing a fixed-length experiment with evolved genomes is desired.
This factor introduces a design subtlety: as generations elapse, deposited strata recede to increasingly ancient historical depth with respect to the current generation.
Resolution guarantees may change along the way back.
In those cases, cohorts of retained strata must, in dwindling, gracefully morph through a constrained series of retention patterns.

\subsection{The streaming curation problem}

Stepping back, the online filtering obligations faced by hereditary stratigraph annotations are not unlike those faced by unattended data logger devices.
Both manage incoming observation streams on a potentially indefinite or indeterminate basis and both operate under storage space limitations.
Further, both are presumedly tasked to operate under some stipulation for time coverage, whether simply rolling full retention of most recent data within available buffer space \citep{fincham1995use}, dismissal of incoming data after storage reaches capacity \citep{saunders1989portable,mahzan2017design}, best-effort even coverage of the elapsed period, or otherwise.
Even high-capacity devices may experience overflow conditions when confronted with high-frequency data streams \citep{luharuka2003design}.

Restated explicitly, these scenarios confront a question of how to satisfactorily maintain a temporally representative cross section of observations that stream in on a rolling basis.
We propose this problem as the ``stream curation problem.''
For the sake of generalizability, Section \ref{sec:annotation-algorithms} organizes presentation of stratum retention algorithms through the lens of the streaming curation problem.
There has been some work to extend the record capacity of data loggers through application-specific online compression algorithms \citep{hadiatna2016design}, but to our knowledge the streaming curation problem has not been treated directly from either a theoretical or empirical vantage.


Some of the same curation policy algorithms we propose for stratum retention could also be useful in these cases.
For example, organization of IoT devices into wireless sensor networks is expected, in a considerable fraction of cases, to structure irregular device uplink schedules, such as the ``mobile sink'' paradigm \citep{jain2022survey}.
Under this model, network base station(s) physically traverse the coverage area and transact with nearby sensor nodes.
Reliance on the mobile sink's patrol schedule potentially introduces uncertainty in data transfer schedules.

Recency-proportional retention may suit some applications, where time intervals of interest may be flagged well after the fact, but tend to bias toward the recent past.
Finally, streaming curation may even pertain to record management in large capacity centralized storage systems in some scenarios \citep{bhat2018data}.

It should also be noted that beyond connections to streaming curation, hereditary stratigraphy itself bears upon applications outside digital evolution modeling.
Computer viruses and digital media also proliferate through replication, often outside the scope of direct centralized observability.
Furthermore, understanding the dynamics of these phenomena is of significant scientific and societal interest \citep{aslan2020comprehensive,dupuis2019spread,ling2021dissecting}.
Indeed, some research has reported on the routes through which digital image misinformation and computer viruses spread \citep{friggeri2014rumor,cohen1987computer}.
Such studies generally rely on centralized tracking, which is not possible in many circumstances.
A notable exception is work by \cite{libennowell2008tracing} to trace global dissemination of chain emails.
These researchers applied \textit{post-hoc} methods to reconstruct estimated phylogenies of the propagation of two chain mail messages.
These phylogenies then served as a reference to tune agent-based models, ultimately yielding a remarkable means elucidation of email user behavior and underlying social dynamics.
Interestingly, this study's reconstructions were solely enabled by a special peculiarity of the two sampled messages: they were email petitions.
Thus, users would append their name to the list before forwarding it on --- a mechanism strikingly similar to the broad strokes of hereditary stratigraphy.

As such, hereditary stratigraphic techniques certainly possess some potential afford experimenters visibility into the otherwise cloistered processes through which such digital artifacts spread.
Notably, though, such experiments would predicate on some level of influence over the artifact copy process in to ensure dispatch of the hereditary stratigraphic update process and the absence of actors with motivation and capability to antagonistically deface annotation data.

Remaining exposition in this paper is structured as follows:
\begin{itemize}
% \item Section \ref{sec:methods} covers preliminaries and glossarizes key terminology,
\item Section \ref{sec:annotation-algorithms} surveys a suite of streaming curation algorithms, introducing intuition, presenting the formal definition, proving self-consistent stratum discard sequencing, and demonstrating resolution and collection size properties.
% TODO make sure this gets into the actual text
\item Section \ref{sec:reconstruction-algorithm} presents a recently-developed algorithm for full-tree reconstruction from hehereditary stratigraphic annotation data and analyzes its runtime characteristics.
\item Section \ref{sec:perfect-tracking-algorithm} supplies formal presentation of the alternate perfect phylogenetic tracking algorithm and analysis of its runtime characteristics then commentates on which situations better suit perfect tracking over hereditary stratigraphy, and vice versa.
% TODO do we need a results and discussion section???
% \item Section \ref{sec:results-and-discussion}
\item Section \ref{sec:conclusion} reflects on broader implications and future work, and
\item we include a Glossary of terminology related to hereditary stratigraphy, streaming curation, and phylogenetics in the Appendix.
\end{itemize}
