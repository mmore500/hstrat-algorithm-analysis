\section{Introduction} \label{sec:introduction}

Contemporary advances in the engineering and manufacture of compute hardware largely come as a double-edged sword.
Fundamental physical limitations have essentially curtailed transformative progress in capabilities of individual processing elements \citep{sutter2005free}.
Instead, the frontier of raw compute power in high-performance computing (HPC) has largely been driven forward by scale-out to multi- and many-core architectures \citep{morgenstern2021unparalleled}.
On the flip side, economization and miniaturization has driven broad dissemination of computing capability through the Internet of Things (IoT) revolution \citep{rfc7228,ojo2018review}.
A notable pattern cuts across some facets of both fringes: absolute reduction or tightening or relative tightening in resources and capabilities per processing element, particularly with respect to the capacity, latency, or bandwidth of memory and storage.

Along the march to exascale HPC, processing capacity growth (e.g., FLOPS) has outpaced memory performance and capacity --- particularly with respect to hardware accelerators \citep{kogge2013exascale,khan2021analysis}.
This ``memory wall'' bottlenecks throughput unless memory access volume is kept sufficiently low in proportion to processing workload (i.e., high ``arithmetic'' or ``operational'' intensity is achieved) \citep{obviousmemorywall,mckee2004reflections,ROOFLINEMODEL2008Berkley}.
Analogous performance trajectories have played out with respect to data storage \citep{heldens2020landscape,kunkel2014exascale}, imposing an analogous ``storage wall'' bottleneck \citep{hu2016storage}.
Recent introduction of the Cerebras Wafer-Scale Engine epitomizes the double-edged nature of recent HPC trends: this accelerator packages an astounding 850,000 computing elements onto a single piece of silicon, but restricts endows each with a modest 48kb of memory and provisions an exclusively-local mesh communication model \citep{cerebras2021wafer,lauterbach2021path}.

Low-end hardware, on the other hand, marches toward the downscale and ubiquity of ``smart dust'' \citep{warneke2001smart}.
The ``Michigan Micro Mote'' constitutes a notable waypoint \citep{lee2012modular}.
These devices occupy only a cubic millimeter of volume, but provision a mere 3 kb of retentive memory.
Introduced in 2012, applications have since been explored across imaging, motion detection, pressure measurement, and electrocardiography monitoring \citep{lee2015review}.
Taking for instance another more recent high-profile project, computational capacity was recently tucked into wireless sensor network devices adopting the form factor of dandelion parachutes \citep{iyer2022wind}.
The chipset for these devices provisions 2 kilobytes of volatile flash memory --- and a mere 128 bytes of retentive memory \citep{microchip2014atiny20}.
% Even more striking is the advent of an epidermal physiological sensor equipped with carbon nanotube-based flash memory of a scant 48 bytes \citep{xiang2022epidermal}.
The ever abiding race to plumb the very lower limits of technical feasibility seems likely to keep bare-bones computing modalities front and center within this engineering domain far into the future.

The intersection of these observations is that despite relentless technological advancement, severely resource-limited computation will persist.
In fact, in some domains technological progress has actually elevated the relevance of resource-limited computation.
These trends, and the wider context of surging generation of digital data \citep{bhat2018data}, demand attention to algorithmic strategies to triage memory/storage-capacity gaps.

Burgeoning computational power has been broadly envisaged as driving force in advancing the field of digital evolution and, more broadly, artificial life \citep{ackley2014indefinitely}.
In fact, a central motif in the field --- the problem of open-ended evolution, how and if closed systems can yield indefinitely yield new complexity, novelty, and adaptation --- is inexorably intertwined with computational scale, and has been implicated as meaningfully compute-constrained in at least some systems \citep{taylor2016open,channon2019maximum}.
It is not an unreasonable possibility that orders-of-magnitude changes in computational power could induce qualitative changes in digital evolution and artificial life systems \citep{moreno2022engineering}, analogously to the renaissance of deep learning with the advent of GPGPU computing \citep{krizhevsky2012imagenet}.
To accomplish such, however, will require contending with scaling challenges induced by aforementioned hardware trends.

Parallel and distributed computing is, indeed, widely leveraged in digital evolution.
Applications range from processing entirely independent evolutionary replicates across compute jobs \citep{dolson2017spatial, hornby2006automated}, to data-parallel fitness evaluation of single individuals over independent test cases using hardware accelerators \citep{harding2007fast_springer, langdon2019continuous}, to application of primary-subordinate/controller-responder parallelism to delegate costly fitness evaluations of single individuals \citep{cantu2001master,miikkulainen2019evolving}.
In recent years, Sentient Technologies spearheaded evolutionary computation projects on an unprecedented computational scale, comprising over a million heterogeneous CPUs from time-available providers capable of a peak performance of 9 petaflops \citep{miikkulainen2019evolving,gilbert2015artificial,blondeau2009distributed}.

Although highlighted as key to the future of the field, fully decentralized, highly-distributed approaches such as island models \citep{bennett1999building,schulte2010genetic} and mesh-computing approaches prioritizing dynamic interactions \citep{ray1995proposal,ackley2018digital,moreno2021conduit} have been less thoroughly developed.
Observability constitutes a major barrier to effective applications of the fully-distributed paradigm: in the absence of a global population-level view, the course of evolution becomes much more challenging to study --- undercutting the objective of these systems as an experimental tool.

Difficulty scaling typical approaches to phylogenetic analyses of digital evolution systems poses a particularly problematic stumbling block.
Most existing analyses leverage a direct lineage tracking approach where a centralized data structure collates individual reproduction events to produce a complete, perfect phylogenetic record (TODO cite phylotrackpy)
%TODO we could probably add some citations/examples about why phylogenetic tracking is important
% AND how perfect phylogenetic tracking is super powerful at doing otherwise unimaginable analyses
% probably pilfer phylotrackpy JOSS paper for ideas
% TODO also tie in the algorithmic analysis of phylogenies here
However, long distance, potentially many-hop, transmission of reproduction event records to a centralized data store introduces potentially significant amounts of communication overhead and bottlenecking.
The necessity to propagate extinction notifications at runtime so that records of lineages without extant descendants can be pruned away to prevent runaway memory use imposes additional communication overhead.
Further, at large enough scales, node failures become inevitablies rather than inconveniences and, at even larger scales, the cost of attempting full recovery from node dropout to prevent data loss could become high enough to conceivably preclude forward simulation progress \citep{cappello2014toward}.
Within the perfect-tracking paradigm, individual missing relationships could potentially entirely disjoin knowledge of how large portions of phylogenetic history relate.
This sensitivity to data loss poses a second major challenge in scaling perfect tracking.

Looking to  natural systems, however, fruitful efforts across generations of systematic biologists evidence the viability of robust phylogenetic analyses from data generated through fully-distributed processes.
An alternate approach to studying phylogenetic dynamics in distributed systems, therefore, can be entertained: post-hoc inference from heritable data rather than centralized tracking.
Although such analyses can passably be accomplished solely from direct analysis of pre-existing, functional digital genome contents \citep{moreno2021case}, such algorithms do not generalize across diverse digital evolution systems and suffer from many of the complications that plague phylogenetic analyses of biological systems to boot (TODO cite).
This line of reasoning led to the recent development of hereditary stratigraphy methodology, which associates digital genomes with heritable annotations specially designed to provide universal system-invariant phylogenetic reconstruction with well-defined, tunable expectations for inference accuracy
\citep{moreno2022hereditary}.

A fuller accounting appears in Section \ref{sec:methods}, but the principal aspects of hereditary stratigraphy can be sketched briefly.
However, briefly sketching a more obvious alternate approach provides a point of reference more familiar to most readrs' intuiition.
A simple heritable annotation to enable relatedness estimation would be a fixed--length bitstring under neutral drift by bit flip mutation.
As lineages diverge from a common ancestor, gradual mutational accumulation probabilistically causes the hamming distance between annotations to diverge.
Therefore, hamming distance between these annotations can be used as inferential proxy: more mismatching sites imply less relatedness between two annotated digital organisms.
Unfortunately, this model suffers from several serious drawbacks in application as a tool for relatedness estimation, including difficulty resolving uneven lineage branch lengths and difficulty parameterizing the model for effective inference over greatly varying generational scales.

Hereditary stratigraphy annotations employ a systematic update process at generational turnovers instead of a stochastic mutational process as described above.
Each generation, a new fixed-length packet of randomly-generated data is appended to inherited annotations.
These stochastic ``fingerprints,'' typically sized on the order of a few bits or bytes, serve as a sort of checkpoint for lineage identity at a particular generation.
At future time points, extant annotations will share identical fingerprints for the generations they experienced shared ancestry.
Estimation of the last common ancestor generation is straightforward: the first mismatching fingerprints denote the end of common ancestry.
However, note that overestimation of relatedness can occur due to spurious fingerprint collision events (i.e., where strata happen to share the same randomly-generated value by chance); the probability of such events depends on ``fingerprint'' width.
The nomenclature of ``hereditary stratigraphy'' was chosen through analogy to geological layering processes.

Putting ``hereditary stratigraphy'' techniques into practice requires a point of further consideration.
Naive application of the approach described above would entail indefinitely lengthening genome annotation size in direct proportion to generations elapsed due to the creation of new ``fingerprint'' data each generation.
To overcome this obstacle, hereditary stratigraphy prescribes a ``pruning'' process to discard strata on the fly as new strata are deposited.
Note that pruning reduces annotation size, but at the expense of introducing inferential uncertainty: the last generation of common ancestry between two lineages can be resolved no finer than the time points associated with retained strata.
Deciding which strata to discard when proves to be the main point of algorithmic interest associated with the technique.
We refer to this decision-making process as a ``stratum retention'' algorithm.

The obvious core issue of stratum retention is how many strata to discard.
In many applied cases, it is desirable to keep the count of retained strata at or below a size cap indefinitely as generations elapse.
However, in some cases discussed later on, it may be desirable to allow a logarithmic growth rate of annotation size to guarantee certain bounds on inferential uncertainty.

Determining stratum retention strategy also obliges a more subtle second consideration: what skew, if any, to induce on the composition of retained strata.
Strata from evenly-spaced time points may retained in order to provide uniform inferential detail over the entire range of elapsed time points.
However, coalescent theory suggests fine inferential detail over more recent time points as likely more informative to phylogenetic reconstruction than fine detail over more ancient time points.
%TODO add citations and lock this down
Thus, among a fixed-size retained sampling of time points, skewing the composition of retained strata to over-represent more recent time points would be expected to provide better bang for the buck with respect to reconstructive power.
Indeed, experiments reconstructing known lineages have shown that recency-skewing retention provides better quality reconstructions \citep{moreno2022hereditary}.
So, in addition to evenly-spaced retention, we consider retention allocations that yield gap widths between successive retained strata (and corresponding estimation uncertainty) scaled proportionately to those strata's depth back in history.

\begin{enumerate}
\item Column composition across intermediate generations.
  For many use cases, resolution and column size guarantees will need to hold at all generations because the number of generations elapsed at the end of an experiment is often not known \textit{a priori} and the option of continuing a fixeed-length experiment with evolved genomes is desired.
\item Prune sequencing.
  When you discard a stratum now, it won't be available later.
  If you need a stratum at a particular time point, you must be able to guarantee it hasn't already been discarded at any prior time point.
\item Tractability of enumerating the deposit generations of retained strata at an arbitrary generation.
  Efficient computation of the deposit generations retained at each time point provides a tractable reverse mapping from column array index to deposition generation.
  Such a mapping enables deposition generation to be omitted from strata, potentially yielding several-fold space savings (depending on the differentia bit width used).
\end{enumerate}


Emerging so-called ``hereditary stratigraphy’’ methodology abstracts this problem to a more general one: how to satisfactorily maintain a temporally representative collection of observations that stream in on a rolling basis.
We term this more general question the ``stream curation problem.’’
Because ongoing data feeds necessitate continual downsampling when faced with storage and memory limitations, the streaming curation problem occurs also in context of unattended data loggers, irregularly-uplinked wireless sensor network devices, and servers under continuous application monitoring, among other scenarios.
To meet requirements of diverse streaming curation scenarios, we present a suite of streaming curation policy algorithms that trade off along a continuum of collection size and temporal coverage.
Policies span $\mathcal{O}(n)$, $\mathcal{O}(\log n)$, and $\mathcal{O}(1)$ orders of growth in curated collection size.
Within each order of growth, policy algorithms differ in relative prioritization of newer versus older data.
We explore two alternatives: even density distribution across history or recency-proportional distribution with greater density at more recent time points.
Policy algorithm designs explicitly pre-determine retained collection composition at each time point, enabling key optimizations to reduce storage overhead and streamline processing of incoming observations.



% suffers
% In analogy to geological layering, we refer to ``fingerprints'' as being contained within a ``stratum'' associated with a particular generation, we refer
% We call the annotation comprised of strata deposited at each generation a hereditary stratigraphic column.




Describe how hstrat works (briefly?)
mention \texttt{hstrat} software library \citep{moreno2022hstrat}



\begin{itemize}
  \item hereditary stratigraphy generalizes to broader question of replicating digital artifacts
  \item digital media and computer viruses
  \item existing studies generally employ centralized tracking
  \begin{itemize}
    \item image/text snippet sharing on facebook \citep{friggeri2014rumor}
    \item Experiments studying the propagation of computer viruses with centralized instrumentation \citep{cohen1987computer} (want more cites)
  \end{itemize}
  \item interesting instance of reconstruction that shares a lot in common with hereditary stratigraphy: chain email sharing \citep{libennowell2008tracing}
\end{itemize}

\begin{itemize}
  \item IoT/wireless sensor networks
  \item low power/capacity hardware: smart dust \citep{warneke2001smart}
  \item run out of space:
  \begin{itemize}
    \item long-duration solar-powered sensor --- battery is not limiting factor \citep{corke2007long}
    \item not just limited onboard memory
    \item high frequency data \citep{luharuka2003design}
  \end{itemize}
  \item common theme: irregular connectivity
  \begin{itemize}
    \item communicication outages/disruptions (need cite)
    \item query-based networks (need cite) (note ``security'' applications where desired content is only known long after the fact?)
    \item mobile sinks \citep{jain2022survey} (for security, power efficiency (fewer hops), reduce bottlenecking)
    \item at the very extreme are autonomous loggers
    \item logger: new data discarded once full \citep{saunders1989portable,mahzan2017design}
    \item logger: new data replaces oldest data (flight data recorder; need cite)
    \item logger: pplication-specific online compression algorithm that (\citep{hadiatna2016design} increases capacity twofold)
  \end{itemize}
  \item more data coming in from everywhere, also need a principled strategy to triage ``record management'' in centralized storage
  \item end on: need a principled way to weed time series data on the fly that meets coverage requirements at any point in time
\end{itemize}



then outline paper
\begin{itemize}
\item intro
\item preliminaries
\item perfect Tracking
\item policies
\item discussion
\end{itemize}
